{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting things in DPLA with SparkSQL\n",
    "\n",
    "Inspired by Corey Harper's [DPLA Analytics](https://github.com/chrpr/dpla-analytics) work, I wanted to get my feet wet working with basic [DPLA metadata structures](http://dp.la/info/developers/map/) using [SparkSQL](http://spark.apache.org/sql/).  This brings together a few things I haven't done before:  working with DPLA metadata from their [bulk downloads](http://dp.la/info/developers/download/), using SparkSQL, and putting the two together.\n",
    "\n",
    "In Corey's [Statistical DPLA: Metadata Counting and Word Analysis](https://open.library.ubc.ca/cIRcle/collections/55474/items/1.0220818) from the fall 2015 [DLF Forum](https://www.diglib.org/forums/2015forum/livestream-schedule/) he discusses the results of his work to characterize DPLA records at the scale of \"all of it\", and to begin to tie that into patterns of use.  In an offline discussion he indicated that one barrier to being able to move faster with this work is the processing time it takes to prep the data to make it amenable to the kind of aggregate summarizations he is performing.  Working at a scale of millions of records, some data prep operations can take hours to complete on a single laptop.\n",
    "\n",
    "These days, I'll readily admit, I've been walking around with Jupyter and Spark hammers, and this problem sounds very much like a Spark-friendly nail.  There are many other approaches to addressing this particular performance problem, but I have grown to enjoy Spark's terrific performance, friendly APIs, and rapid improvements and am always looking to find more ways to work with it.\n",
    "\n",
    "With that in mind, in this notebook I'll demonstrate the basics of acquiring a DPLA JSON dump, prepping it to load into Spark, configuring Jupyter/IPython to connect to Spark, loading the JSON into a schema within Spark, and issuing basic queries.  For the sake of expediency, I'll do this using a small subset of DPLA data; in a later notebook I'll pull up a big server and work with the whole dataset to go a little deeper with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquiring DPLA JSON data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the most recent [DPLA bulk downloads](http://dp.la/info/developers/download/?prefix=2016/02/), we find a 46.8MB (compressed) record set from the Getty called [getty.json.gz](https://dpla-provider-export.s3.amazonaws.com/2016/02/getty.json.gz).  Download that to your local disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-02-29 13:33:48--  https://dpla-provider-export.s3.amazonaws.com/2016/02/getty.json.gz\n",
      "Resolving dpla-provider-export.s3.amazonaws.com... 54.231.81.80\n",
      "Connecting to dpla-provider-export.s3.amazonaws.com|54.231.81.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 49112071 (47M) [application/gzip]\n",
      "Saving to: 'getty.json.gz'\n",
      "\n",
      "getty.json.gz       100%[===================>]  46.84M  1.20MB/s    in 40s     \n",
      "\n",
      "2016-02-29 13:34:29 (1.16 MB/s) - 'getty.json.gz' saved [49112071/49112071]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dpla-provider-export.s3.amazonaws.com/2016/02/getty.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see just how many records that contains (note: ```gzcat``` because I'm on OSX; you might want ```zcat``` instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   95910\r\n"
     ]
    }
   ],
   "source": [
    "!gzcat getty.json.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like over 95,000 records.  Great.  Onto the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping DPLA JSON data to load into Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark prefers to load line-oriented JSON.  Line-oriented JSON looks like this:\n",
    "\n",
    "```\n",
    "{'id': 1, 'data': ...}\n",
    "{'id': 2, 'data': ...}\n",
    "{'id': 3, 'data': ...}\n",
    "... more ...\n",
    "```\n",
    "\n",
    "...which is to say \"one record per line\".  This makes it amenable to piping and streaming operations.  So far so good.\n",
    "\n",
    "Note, though, that the [DPLA data doesn't come like that](https://digitalpubliclibraryofamerica.atlassian.net/wiki/display/TECH/Database+export+files) -- rather, bulk download files like the one we just grabbed look more like this:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        ...\n",
    "        \"_source\": { ... record ... }\n",
    "        ...\n",
    "    },\n",
    "    ... more ...\n",
    "]\n",
    "```\n",
    "\n",
    "And that's with a little whitespace added for clarity.  Removing that extra whitespace, they actually look like this:\n",
    "\n",
    "```\n",
    "[\n",
    "{...,\"_source\": { ... record ... },...}\n",
    ",{...,\"_source\": { ... record ... },...}\n",
    ",{...,\"_source\": { ... record ... },...}\n",
    "... more ...\n",
    "]\n",
    "```\n",
    "\n",
    "Note that this is a JSON serialization of a set of records, not line-oriented JSON where each line contains one record.  To work with this data, a JSON deserializer will have to read the entire set into memory before proceeding.  This isn't really a problem, it's just a slightly different approach, and not the one Spark prefers.  Fortunately, we can easily change the recordset-oriented data into line-oriented records.  There are two issues we have to deal with, first being the opening and closing square brackets, and the second being the leading commas.  \n",
    "\n",
    "Unix to the rescue here.  We can use ```head```, ```tail```, and ```sed``` to handle it.  We just need to know how many lines there are total so we know how many to include.  And we already know that from the above:  95,910.  This implies (skipping the first and last lines) that there are actually 95,908 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!gzcat getty.json.gz | head -95909 | tail -95908 > getty-nobrackets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get rid of the leading commas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sed -e 's/^,//' getty-nobrackets.json > getty-lines.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   95908 getty-lines.json\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l getty-lines.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right.  On to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Jupyter/IPython to connect to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Spark installed\n",
    "\n",
    "This part can be tricky, but I think I've got it boiled down to just a few steps.  First, I'm assuming you have Spark installed.  If you don't, it's not hard.  On OSX with [homebrew](http://brew.sh):\n",
    "\n",
    "```\n",
    "% brew install apache-spark\n",
    "```\n",
    "\n",
    "On other platforms, you might need to [download Spark](http://spark.apache.org/downloads.html) and install it.  The bundles pre-built for Hadoop make it very easy.  Just unwrap the bundle and follow the instructions - you'll be surprised how easy it is to get started.  We're going to use ```pyspark```, which is found in the ```bin``` directory of the Spark distribution you've grabbed.\n",
    "\n",
    "At this point, I wave my hands, and assume that you are able to type...:\n",
    "\n",
    "```\n",
    "% pyspark\n",
    "```\n",
    "\n",
    "...and see a few dozen INFO lines scroll by, ending with this:\n",
    "\n",
    "```\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 2.7.11 (default, Feb 15 2016 13:40:02)\n",
    "SparkContext available as sc, HiveContext available as sqlContext.\n",
    ">>>\n",
    "```\n",
    "\n",
    "And at this point, you can type ```sqlContext```, and should get the following output:\n",
    "\n",
    "```\n",
    ">>> sqlContext\n",
    "<pyspark.sql.context.HiveContext object at 0x10c1d1d10>\n",
    "```\n",
    "\n",
    "If that's what you get, you're all set!\n",
    "\n",
    "(Note that bit about ```sc``` and ```sqlContext``` - we'll use the latter for SparkSQL in a moment, and the former gives you access to anything else non-SQL in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Jupyter installed\n",
    "\n",
    "The easiest way to get the Jupyter notebook installed is through Continuum's [Anaconda Python package](https://www.continuum.io/downloads).  Follow their instructions and you'll get a fully working scientific Python environment in a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Jupyter to Spark\n",
    "\n",
    "I found [these instructions](http://stackoverflow.com/questions/33064031/link-spark-with-ipython-notebook/33065359#33065359) to be the simplest way to connect Jupyter to Spark.  Rather than make you parse them line-by-line, here are the key bits:\n",
    "\n",
    "```\n",
    "% export PATH=/WHERE/YOU/PUT/SPARK/bin:$PATH\n",
    "% export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "% export PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark\n",
    "```\n",
    "\n",
    "A few notes on this:  first, substitute where you actually unwrapped Spark for ```/WHERE/YOU/PUT/SPARK```; second, the notebook is now called \"Jupyter\", not \"ipython\", so setting ```PYSPARK_DRIVER_PYTHON``` to ```jupyter``` instead of ```ipython``` will save a few warnings; finally, the last bit assures you that whenever you start ```pyspark``` (like you did above) from now on, it will start and open a new Jupyter notebook for you, rather than the command line shell you saw before.\n",
    "\n",
    "If you like all that, you should add those three lines to your ```.profile``` so your shell will be configured to do it by default, as that Stack Overflow answer suggests.  Doing it the way I listed it here means it only works during your current shell session.  It's up to you.\n",
    "\n",
    "Test it all out.  Set those environment variables and then type ```pyspark```:\n",
    "\n",
    "```\n",
    "% pyspark\n",
    "```\n",
    "\n",
    "Your web browser should pop open and a new pyspark notebook should be available.  In the first cell, enter ```sqlContext```, execute it, and you should see the same kind of output you saw above when you did the same thing on the command line.  It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.HiveContext at 0x1116ec2e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got it?  Good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading DPLA JSON data into Spark\n",
    "\n",
    "Getting the line-oriented data into Spark is easier that you think.  It might take a minute or two, though, so don't panic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getty = sqlContext.read.json(\"getty-lines.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it.  The data is now ready for Spark to process.\n",
    "\n",
    "SparkSQL first will infer a schema from the JSON as it appeared in the record set.  We can examine the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- _index: string (nullable = true)\n",
      " |-- _score: long (nullable = true)\n",
      " |-- _source: struct (nullable = true)\n",
      " |    |-- @context: string (nullable = true)\n",
      " |    |-- @id: string (nullable = true)\n",
      " |    |-- @type: string (nullable = true)\n",
      " |    |-- _id: string (nullable = true)\n",
      " |    |-- _rev: string (nullable = true)\n",
      " |    |-- admin: struct (nullable = true)\n",
      " |    |    |-- sourceResource: struct (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- valid_after_enrich: boolean (nullable = true)\n",
      " |    |    |-- validation_message: string (nullable = true)\n",
      " |    |-- aggregatedCHO: string (nullable = true)\n",
      " |    |-- dataProvider: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- ingestDate: string (nullable = true)\n",
      " |    |-- ingestType: string (nullable = true)\n",
      " |    |-- ingestionSequence: long (nullable = true)\n",
      " |    |-- isShownAt: string (nullable = true)\n",
      " |    |-- object: string (nullable = true)\n",
      " |    |-- originalRecord: struct (nullable = true)\n",
      " |    |    |-- ID: string (nullable = true)\n",
      " |    |    |-- NO: string (nullable = true)\n",
      " |    |    |-- PrimoNMBib: struct (nullable = true)\n",
      " |    |    |    |-- record: struct (nullable = true)\n",
      " |    |    |    |    |-- addata: struct (nullable = true)\n",
      " |    |    |    |    |    |-- abstract: string (nullable = true)\n",
      " |    |    |    |    |    |-- addau: string (nullable = true)\n",
      " |    |    |    |    |    |-- addtitle: string (nullable = true)\n",
      " |    |    |    |    |    |-- au: string (nullable = true)\n",
      " |    |    |    |    |    |-- btitle: string (nullable = true)\n",
      " |    |    |    |    |    |-- date: string (nullable = true)\n",
      " |    |    |    |    |    |-- format: string (nullable = true)\n",
      " |    |    |    |    |    |-- genre: string (nullable = true)\n",
      " |    |    |    |    |    |-- pub: string (nullable = true)\n",
      " |    |    |    |    |    |-- risdate: string (nullable = true)\n",
      " |    |    |    |    |    |-- ristype: string (nullable = true)\n",
      " |    |    |    |    |-- browse: struct (nullable = true)\n",
      " |    |    |    |    |    |-- author: string (nullable = true)\n",
      " |    |    |    |    |    |-- subject: string (nullable = true)\n",
      " |    |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |    |-- control: struct (nullable = true)\n",
      " |    |    |    |    |    |-- recordid: string (nullable = true)\n",
      " |    |    |    |    |    |-- sourceformat: string (nullable = true)\n",
      " |    |    |    |    |    |-- sourceid: string (nullable = true)\n",
      " |    |    |    |    |    |-- sourcerecordid: string (nullable = true)\n",
      " |    |    |    |    |    |-- sourcesystem: string (nullable = true)\n",
      " |    |    |    |    |-- delivery: struct (nullable = true)\n",
      " |    |    |    |    |    |-- delcategory: string (nullable = true)\n",
      " |    |    |    |    |-- display: struct (nullable = true)\n",
      " |    |    |    |    |    |-- contributor: string (nullable = true)\n",
      " |    |    |    |    |    |-- coverage: string (nullable = true)\n",
      " |    |    |    |    |    |-- creationdate: string (nullable = true)\n",
      " |    |    |    |    |    |-- creator: string (nullable = true)\n",
      " |    |    |    |    |    |-- format: string (nullable = true)\n",
      " |    |    |    |    |    |-- ispartof: string (nullable = true)\n",
      " |    |    |    |    |    |-- language: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds01: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds03: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds04: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds07: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds09: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds10: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds14: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds26: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds27: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds28: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds29: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds32: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds34: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds43: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds45: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds47: string (nullable = true)\n",
      " |    |    |    |    |    |-- lds49: string (nullable = true)\n",
      " |    |    |    |    |    |-- publisher: string (nullable = true)\n",
      " |    |    |    |    |    |-- rights: string (nullable = true)\n",
      " |    |    |    |    |    |-- source: string (nullable = true)\n",
      " |    |    |    |    |    |-- subject: string (nullable = true)\n",
      " |    |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |    |    |-- facets: struct (nullable = true)\n",
      " |    |    |    |    |    |-- creationdate: string (nullable = true)\n",
      " |    |    |    |    |    |-- creatorcontrib: string (nullable = true)\n",
      " |    |    |    |    |    |-- frbrgroupid: string (nullable = true)\n",
      " |    |    |    |    |    |-- frbrtype: string (nullable = true)\n",
      " |    |    |    |    |    |-- genre: string (nullable = true)\n",
      " |    |    |    |    |    |-- language: string (nullable = true)\n",
      " |    |    |    |    |    |-- lfc02: string (nullable = true)\n",
      " |    |    |    |    |    |-- lfc03: string (nullable = true)\n",
      " |    |    |    |    |    |-- lfc05: string (nullable = true)\n",
      " |    |    |    |    |    |-- prefilter: string (nullable = true)\n",
      " |    |    |    |    |    |-- rsrctype: string (nullable = true)\n",
      " |    |    |    |    |    |-- topic: string (nullable = true)\n",
      " |    |    |    |    |    |-- toplevel: string (nullable = true)\n",
      " |    |    |    |    |-- links: struct (nullable = true)\n",
      " |    |    |    |    |    |-- linktorsrc: string (nullable = true)\n",
      " |    |    |    |    |    |-- lln02: string (nullable = true)\n",
      " |    |    |    |    |    |-- lln03: string (nullable = true)\n",
      " |    |    |    |    |    |-- lln04: string (nullable = true)\n",
      " |    |    |    |    |    |-- openurlfulltext: string (nullable = true)\n",
      " |    |    |    |    |    |-- thumbnail: string (nullable = true)\n",
      " |    |    |    |    |-- ranking: struct (nullable = true)\n",
      " |    |    |    |    |    |-- booster1: string (nullable = true)\n",
      " |    |    |    |    |    |-- booster2: string (nullable = true)\n",
      " |    |    |    |    |-- search: struct (nullable = true)\n",
      " |    |    |    |    |    |-- addtitle: string (nullable = true)\n",
      " |    |    |    |    |    |-- alttitle: string (nullable = true)\n",
      " |    |    |    |    |    |-- creationdate: string (nullable = true)\n",
      " |    |    |    |    |    |-- creatorcontrib: string (nullable = true)\n",
      " |    |    |    |    |    |-- description: string (nullable = true)\n",
      " |    |    |    |    |    |-- enddate: string (nullable = true)\n",
      " |    |    |    |    |    |-- format: string (nullable = true)\n",
      " |    |    |    |    |    |-- general: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |    |-- lsr08: string (nullable = true)\n",
      " |    |    |    |    |    |-- lsr32: string (nullable = true)\n",
      " |    |    |    |    |    |-- lsr34: string (nullable = true)\n",
      " |    |    |    |    |    |-- recordid: string (nullable = true)\n",
      " |    |    |    |    |    |-- rsrctype: string (nullable = true)\n",
      " |    |    |    |    |    |-- scope: string (nullable = true)\n",
      " |    |    |    |    |    |-- searchscope: string (nullable = true)\n",
      " |    |    |    |    |    |-- sourceid: string (nullable = true)\n",
      " |    |    |    |    |    |-- startdate: string (nullable = true)\n",
      " |    |    |    |    |    |-- subject: string (nullable = true)\n",
      " |    |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |    |-- sort: struct (nullable = true)\n",
      " |    |    |    |    |    |-- author: string (nullable = true)\n",
      " |    |    |    |    |    |-- creationdate: string (nullable = true)\n",
      " |    |    |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |    |-- xmlns: string (nullable = true)\n",
      " |    |    |-- RANK: string (nullable = true)\n",
      " |    |    |-- SEARCH_ENGINE: string (nullable = true)\n",
      " |    |    |-- SEARCH_ENGINE_TYPE: string (nullable = true)\n",
      " |    |    |-- _id: string (nullable = true)\n",
      " |    |    |-- collection: struct (nullable = true)\n",
      " |    |    |    |-- @id: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- provider: struct (nullable = true)\n",
      " |    |    |    |-- @id: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- sear:GETIT: struct (nullable = true)\n",
      " |    |    |    |-- GetIt1: string (nullable = true)\n",
      " |    |    |    |-- GetIt2: string (nullable = true)\n",
      " |    |    |    |-- deliveryCategory: string (nullable = true)\n",
      " |    |    |-- sear:LINKS: struct (nullable = true)\n",
      " |    |    |    |-- sear:linktorsrc: string (nullable = true)\n",
      " |    |    |    |-- sear:lln02: string (nullable = true)\n",
      " |    |    |    |-- sear:lln03: string (nullable = true)\n",
      " |    |    |    |-- sear:lln04: string (nullable = true)\n",
      " |    |    |    |-- sear:openurlfulltext: string (nullable = true)\n",
      " |    |    |    |-- sear:thumbnail: string (nullable = true)\n",
      " |    |-- provider: struct (nullable = true)\n",
      " |    |    |-- @id: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |-- sourceResource: struct (nullable = true)\n",
      " |    |    |-- @id: string (nullable = true)\n",
      " |    |    |-- collection: struct (nullable = true)\n",
      " |    |    |    |-- @id: string (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- contributor: string (nullable = true)\n",
      " |    |    |-- creator: string (nullable = true)\n",
      " |    |    |-- date: struct (nullable = true)\n",
      " |    |    |    |-- begin: string (nullable = true)\n",
      " |    |    |    |-- displayDate: string (nullable = true)\n",
      " |    |    |    |-- end: string (nullable = true)\n",
      " |    |    |-- description: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- extent: string (nullable = true)\n",
      " |    |    |-- format: string (nullable = true)\n",
      " |    |    |-- identifier: string (nullable = true)\n",
      " |    |    |-- language: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- iso639_3: string (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- publisher: string (nullable = true)\n",
      " |    |    |-- relation: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- rights: string (nullable = true)\n",
      " |    |    |-- specType: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- subject: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- title: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- _type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getty.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to tell SparkSQL that we're going to be using these records in SQL queries for the rest of our session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getty.registerTempTable(\"getty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we're ready to issue some SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issuing basic queries using SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on out, it's pretty easy - if you know SQL, you're pretty much ready to go.  The key thing to remember, though, is that SparkSQL results come out in a [DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes) and you'll have to use DataFrame conventions to view them.  Also, remember that every Spark processing job has a little overhead.  You pay the price of a slow setup to gain the benefit of huge speedups Spark provides by running your job in memory and with multiple CPUs.  We'll see this payoff more when we're working with much more on a beefy machine, but in the meantime, just sit tight and wait for the job to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|the_count|\n",
      "+---------+\n",
      "|    95908|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = sqlContext.sql(\"SELECT COUNT(*) AS the_count FROM getty\")\n",
    "count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks right!\n",
    "\n",
    "For something a little more interesting, let's group and count by object type.  Here we run into another issue we inherit from DPLA's metadata structure:  field names starting with ```@``` and ```_``` should be escaped using backticks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|            type|the_count|\n",
      "+----------------+---------+\n",
      "|           image|    92190|\n",
      "|[\"image\",\"text\"]|     2616|\n",
      "|            text|     1094|\n",
      "|            null|        3|\n",
      "|    moving image|        3|\n",
      "| physical object|        1|\n",
      "|           sound|        1|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "types = sqlContext.sql(\"\"\"\n",
    "SELECT `_source`.sourceResource.type AS type, COUNT(*) AS the_count \n",
    "FROM getty \n",
    "GROUP BY `_source`.sourceResource.type\n",
    "ORDER BY the_count DESC\n",
    "\"\"\")\n",
    "types.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too surprisingly for materials from the Getty, most of the records are some form of image.\n",
    "\n",
    "Ah, here's an issue - some records have multi-valued results.  We'd need to flatten that to count more carefully, let's revisit that later.\n",
    "\n",
    "For now, thought, let's try another, looking more closely at dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| displayDate|count|\n",
      "+------------+-----+\n",
      "|   1960-1990|73646|\n",
      "|      [187-]|  718|\n",
      "|      [188-]|  431|\n",
      "|        1951|  306|\n",
      "|      [186-]|  291|\n",
      "|        1949|  277|\n",
      "|        1953|  274|\n",
      "|c. 1675-1725|  269|\n",
      "|c. 1500-1525|  260|\n",
      "|        1954|  250|\n",
      "|        1950|  246|\n",
      "|        1952|  235|\n",
      "|        1948|  209|\n",
      "|        1956|  204|\n",
      "|        1961|  199|\n",
      "|        null|  199|\n",
      "|        1963|  192|\n",
      "|        1955|  190|\n",
      "|        1964|  185|\n",
      "|c. 1700-1725|  184|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates = sqlContext.sql(\"\"\"\n",
    "SELECT `_source`.sourceResource.date.displayDate, COUNT(*) AS count\n",
    "FROM getty\n",
    "GROUP BY `_source`.sourceResource.date.displayDate\n",
    "ORDER BY count DESC\n",
    "LIMIT 20\n",
    "\"\"\")\n",
    "dates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, bibliographic metadata with its uncertainties and syntax.  Ain't it a joy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing notes\n",
    "\n",
    "On my machine these queries took a number of seconds to execute.  With Spark, it's often that last bit -- here, the ```.show()``` call -- that forces Spark to execute the commands you've prepped it to perform.  In this case, the SQL command isn't executed until the ```show()``` command is given.  This allows you to build pipelined operations on DataFrames, which we can get into more some other time.\n",
    "\n",
    "One of the fun things with Spark is to see what's going on under the hood.  It has its own web interface.  If you're running Spark on your local machine like me, you can visit [localhost:4040](http://localhost:4040) and see the interface for yourself.  Click on the most recently-completed job, view the event timeline, open the DAG visualization, and click further into each stage to see how Spark handled it all.  What you'll see there is why Spark is one of my favorite hammers these days - it'll handle all that complex job tasking for you, and all you have to do is issue a few declarative (in the case of SQL, or functional in a lot of other cases) commands.  Spark will spread those jobs over all the hardware it has available, and pull all the results back together for you too.\n",
    "\n",
    "Looking at that last job, on my very old laptop (a late 2011 MacBook Air) it took about 20 seconds to process that last query.  If you compare it to what you can get from a good database like MySQL, PostgreSQL, Oracle, or SQL Server, that's terrible.  Really, it's awful.  But think about what you didn't have to do:  define a schema, convert the JSON data, write a custom importer, define and build indexes.  For some quick processing of 90,000 records, the time savings in avoiding those steps can make it worthwhile to go the Spark route.  And if you follow a similar process but on a better machine with more RAM, more CPUs, and faster RAM and CPU than what I have, you get a lot more speed on larger jobs just by having better hardware.  Of course you can also take advantage of similar benefits from good RDBMS systems, but the balance of your time investment will vary.\n",
    "\n",
    "In any case, it's good to have multiple options."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
