{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bikeshare data wrangling with bash\n",
    "\n",
    "Working with [Capital Bikeshare trip history data](http://www.capitalbikeshare.com/trip-history-data) is a lot of fun and fascinating for a DC resident and bikeshare member like me.  So much so that in the past I've worked up a [skeletal app](https://github.com/dchud/bikestat) that makes it easy to load trip data into a database and browse it on the web.  That app could use a little attention, but right now I'm trying to do something different with the same data, and I'm finding that I can get a lot done just by sticking with command line tools. Apparently this is such a good idea that somebody really smart already had it and published [Data Science at the Command Line](http://datascienceatthecommandline.com/), which I'm reading now.  I recommend you take a look at his site and his book to learn a lot more.\n",
    "\n",
    "For now, though, let's look at some quick and easy ways to clean up the data and process it in chunks.  When we're done, we can pull the chunks together to render a map showing the growth of bikeshare docking stations.\n",
    "\n",
    "Let's start by getting some data.  This will grab all the data through bikeshare's first twelve quarters, through summer 2013.  It's a good couple dozen MB so it might take a little time, depending on your connection.\n",
    "\n",
    "NB:  This whole writeup is wordy and rather inefficient.  Several steps could have been faster with combined pipelines or scripts, and it creates hundreds of redundant files.  I'm still getting used to documenting my work in notebooks, though, so please just chalk it up to notebook newbie awkwardness.  I'm not sure what's causing the ```broken pipe``` messages; it might be something in the [bash_kernel](https://github.com/takluyver/bash_kernel) or more likely something I'm not doing correctly in my setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# download the files, -q means \"quiet\", used here only to streamline text\n",
    "wget -q http://www.capitalbikeshare.com/assets/files/trip-history-data/2010-Q4-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2011-Q1-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2011-Q2-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2011-Q3-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2011-Q4-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2012-Q1-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2012-Q2-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2012-Q3-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2012-Q4-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2013-Q1-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2013-Q2-Trips-History-Data.zip \\\n",
    "     http://www.capitalbikeshare.com/assets/files/trip-history-data/2013-Q3-Trips-History-Data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to have these compressed; csv files tend to compress well and save space.  I prefer to keep files zipped up using gzip instead of zip, though, and switching them over offers a change to do a little bash script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2010-Q4-Trips-History-Data.zip\r\n",
      "  inflating: 2010-Q4-Trips-History-Data.csv  \r\n",
      "Archive:  2011-Q1-Trips-History-Data.zip\r\n",
      "  inflating: 2011-Q1-Trips-History-Data.csv  \r\n",
      "Archive:  2011-Q2-Trips-History-Data.zip\r\n",
      "  inflating: 2011-Q2-Trips-History-Data.csv  \r\n",
      "Archive:  2011-Q3-Trips-History-Data.zip\r\n",
      "  inflating: 2011-Q3-Trips-History-Data.csv  \r\n",
      "Archive:  2011-Q4-Trips-History-Data.zip\r\n",
      "  inflating: 2011-Q4-Trips-History-Data.csv  \r\n",
      "Archive:  2012-Q1-Trips-History-Data.zip\r\n",
      "  inflating: 2012-Q1-Trips-History-Data.csv  \r\n",
      "Archive:  2012-Q2-Trips-History-Data.zip\r\n",
      "  inflating: 2012-Q2-Trips-History-Data.csv  \r\n",
      "Archive:  2012-Q3-Trips-History-Data.zip\r\n",
      "  inflating: 2012-Q3-Trips-History-Data.csv  \r\n",
      "Archive:  2012-Q4-Trips-History-Data.zip\r\n",
      "  inflating: 2012-Q4-Trips-History-Data.csv  \r\n"
     ]
    }
   ],
   "source": [
    "# unzip each file, then re-compress with gzip\n",
    "for f in 201*.zip\n",
    "do\n",
    "  unzip $f\n",
    "  gzip `basename $f .zip`.csv\n",
    "  \\rm $f\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're with me so far, you now should have something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 dchud  405867214   1.8M Jan 20 17:26 2010-Q4-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   2.2M Jan 20 17:52 2011-Q1-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   5.5M Jan 20 17:51 2011-Q2-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   6.0M Jan 20 17:45 2011-Q3-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   4.7M Jan 20 17:44 2011-Q4-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   5.2M Jan 20 17:58 2012-Q1-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   8.2M Jan 20 18:03 2012-Q2-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   9.2M Jan 20 18:04 2012-Q3-Trips-History-Data.csv.gz\r\n",
      "-rw-r--r--  1 dchud  405867214   7.0M Jan 20 18:05 2012-Q4-Trips-History-Data.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh 201*.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gzipped files are great to work with because they're easy to work with on the commandline.  ```zcat``` (or ```gzcat```, which I'm using here on an OSX machine) let you stream the uncompressed contents as if it weren't compressed, letting you work with the files without taking up the extra space uncompressing would require.  Let's look at some data using ```gzcat```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration,Start date,End date,Start station,End station,Bike#,Member Type\r",
      "\r\n",
      "14h 26min. 2sec.,12/31/2010 23:49,1/1/2011 14:15,10th & U St NW (31111),10th & U St NW (31111),W00771,Casual\r",
      "\r\n",
      "0h 8min. 34sec.,12/31/2010 23:37,12/31/2010 23:46,10th & U St NW (31111),14th & R St NW (31202),W01119,Registered\r",
      "\r\n",
      "0h 12min. 17sec.,12/31/2010 23:27,12/31/2010 23:39,Park Rd & Holmead Pl NW (31602),14th St & Spring Rd NW (31401),W00973,Registered\r",
      "\r\n",
      "0h 15min. 53sec.,12/31/2010 23:21,12/31/2010 23:37,Calvert St & Woodley Pl NW (31106),14th St & Spring Rd NW (31401),W00914,Registered\r",
      "\r\n",
      "0h 36min. 19sec.,12/31/2010 23:20,12/31/2010 23:56,20th St & Florida Ave NW (31110),Columbus Circle / Union Station (31623),W00859,Casual\r",
      "\r\n",
      "0h 9min. 40sec.,12/31/2010 23:18,12/31/2010 23:28,Lamont & Mt Pleasant NW (31107),10th & U St NW (31111),W01119,Registered\r",
      "\r\n",
      "0h 9min. 56sec.,12/31/2010 23:18,12/31/2010 23:28,Lamont & Mt Pleasant NW (31107),10th & U St NW (31111),W00474,Registered\r",
      "\r\n",
      "0h 39min. 28sec.,12/31/2010 23:17,12/31/2010 23:56,20th St & Florida Ave NW (31110),Columbus Circle / Union Station (31623),W00103,Casual\r",
      "\r\n",
      "4h 35min. 42sec.,12/31/2010 23:16,1/1/2011 3:51,11th & Kenyon St NW (31102),14th St Heights / 14th & Crittenden St NW (31402),W00485,Casual\r",
      "\r\n",
      "gzcat: error writing to output: Broken pipe\r\n",
      "gzcat: 2010-Q4-Trips-History-Data.csv.gz: uncompress failed\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2010-Q4-Trips-History-Data.csv.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gotta love csv.  And if you do love csv, you'll really love [csvkit](http://csvkit.readthedocs.org/), which offers handy tools for working with csv data.  Like csvlook, which makes it easier to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzcat: error writing to output: Broken pipe\r\n",
      "gzcat: 2010-Q4-Trips-History-Data.csv.gz: uncompress failed\r\n",
      "|-------------------+------------------+------------------+------------------------------------+---------------------------------------------------+--------+--------------|\r\n",
      "|  Duration         | Start date       | End date         | Start station                      | End station                                       | Bike#  | Member Type  |\r\n",
      "|-------------------+------------------+------------------+------------------------------------+---------------------------------------------------+--------+--------------|\r\n",
      "|  14h 26min. 2sec. | 12/31/2010 23:49 | 1/1/2011 14:15   | 10th & U St NW (31111)             | 10th & U St NW (31111)                            | W00771 | Casual       |\r\n",
      "|  0h 8min. 34sec.  | 12/31/2010 23:37 | 12/31/2010 23:46 | 10th & U St NW (31111)             | 14th & R St NW (31202)                            | W01119 | Registered   |\r\n",
      "|  0h 12min. 17sec. | 12/31/2010 23:27 | 12/31/2010 23:39 | Park Rd & Holmead Pl NW (31602)    | 14th St & Spring Rd NW (31401)                    | W00973 | Registered   |\r\n",
      "|  0h 15min. 53sec. | 12/31/2010 23:21 | 12/31/2010 23:37 | Calvert St & Woodley Pl NW (31106) | 14th St & Spring Rd NW (31401)                    | W00914 | Registered   |\r\n",
      "|  0h 36min. 19sec. | 12/31/2010 23:20 | 12/31/2010 23:56 | 20th St & Florida Ave NW (31110)   | Columbus Circle / Union Station (31623)           | W00859 | Casual       |\r\n",
      "|  0h 9min. 40sec.  | 12/31/2010 23:18 | 12/31/2010 23:28 | Lamont & Mt Pleasant NW (31107)    | 10th & U St NW (31111)                            | W01119 | Registered   |\r\n",
      "|  0h 9min. 56sec.  | 12/31/2010 23:18 | 12/31/2010 23:28 | Lamont & Mt Pleasant NW (31107)    | 10th & U St NW (31111)                            | W00474 | Registered   |\r\n",
      "|  0h 39min. 28sec. | 12/31/2010 23:17 | 12/31/2010 23:56 | 20th St & Florida Ave NW (31110)   | Columbus Circle / Union Station (31623)           | W00103 | Casual       |\r\n",
      "|  4h 35min. 42sec. | 12/31/2010 23:16 | 1/1/2011 3:51    | 11th & Kenyon St NW (31102)        | 14th St Heights / 14th & Crittenden St NW (31402) | W00485 | Casual       |\r\n",
      "|-------------------+------------------+------------------+------------------------------------+---------------------------------------------------+--------+--------------|\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2010-Q4-Trips-History-Data.csv.gz | head | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And csvcut, which lets you list out and slice-and-dice columns, which will come in handy.  Here are some examples, first getting the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Duration\r\n",
      "  2: Start date\r\n",
      "  3: End date\r\n",
      "  4: Start station\r\n",
      "  5: End station\r\n",
      "  6: Bike#\r\n",
      "  7: Member Type\r\n",
      "gzcat: error writing to output: Broken pipe\r\n",
      "gzcat: 2010-Q4-Trips-History-Data.csv.gz: uncompress failed\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2010-Q4-Trips-History-Data.csv.gz | csvcut -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose just the duration, start, and end stations, referring to the columns by their index as listed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzcat: error writing to output: Broken pipe\r\n",
      "gzcat: 2010-Q4-Trips-History-Data.csv.gz: uncompress failed\r\n",
      "|-------------------+------------------------------------+----------------------------------------------------|\r\n",
      "|  Duration         | Start station                      | End station                                        |\r\n",
      "|-------------------+------------------------------------+----------------------------------------------------|\r\n",
      "|  14h 26min. 2sec. | 10th & U St NW (31111)             | 10th & U St NW (31111)                             |\r\n",
      "|  0h 8min. 34sec.  | 10th & U St NW (31111)             | 14th & R St NW (31202)                             |\r\n",
      "|  0h 12min. 17sec. | Park Rd & Holmead Pl NW (31602)    | 14th St & Spring Rd NW (31401)                     |\r\n",
      "|  0h 15min. 53sec. | Calvert St & Woodley Pl NW (31106) | 14th St & Spring Rd NW (31401)                     |\r\n",
      "|  0h 36min. 19sec. | 20th St & Florida Ave NW (31110)   | Columbus Circle / Union Station (31623)            |\r\n",
      "|  0h 9min. 40sec.  | Lamont & Mt Pleasant NW (31107)    | 10th & U St NW (31111)                             |\r\n",
      "|  0h 9min. 56sec.  | Lamont & Mt Pleasant NW (31107)    | 10th & U St NW (31111)                             |\r\n",
      "|  0h 39min. 28sec. | 20th St & Florida Ave NW (31110)   | Columbus Circle / Union Station (31623)            |\r\n",
      "|  4h 35min. 42sec. | 11th & Kenyon St NW (31102)        | 14th St Heights / 14th & Crittenden St NW (31402)  |\r\n",
      "|-------------------+------------------------------------+----------------------------------------------------|\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2010-Q4-Trips-History-Data.csv.gz | head | csvcut -c 1,4,5 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly useful because the files aren't all in the same csv pattern. Compare 2010-Q4 with 2013-Q4 (which I didn't list in the ```wget``` statement above, and am just showing for the demo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Duration\r\n",
      "  2: Start date\r\n",
      "  3: Start Station\r\n",
      "  4: End date\r\n",
      "  5: End Station\r\n",
      "  6: Bike#\r\n",
      "  7: Subscription Type\r\n",
      "gzcat: error writing to output: Broken pipe\r\n",
      "gzcat: 2013-Q4-Trips-History-Data2.csv.gz: uncompress failed\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2013-Q4-Trips-History-Data2.csv.gz | csvcut -n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that columns 2 and 3 are now the start date/station, and 4 and 5 are the end date/station.  Before they were 2/3 start/end dates, 4/5 start/end stations.  You can rearrange the columns easily with csvcut, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzcat: error writing to output: Broken pipe\r\n",
      "gzcat: 2013-Q4-Trips-History-Data2.csv.gz: uncompress failed\r\n",
      "|-------------+------------------------------------------+-------------------------------------------|\r\n",
      "|  Duration   | Start Station                            | End Station                               |\r\n",
      "|-------------+------------------------------------------+-------------------------------------------|\r\n",
      "|  0h 7m 54s  | New York Ave & 15th St NW                | 23rd & E St NW                            |\r\n",
      "|  0h 26m 23s | Rosslyn Metro / Wilson Blvd & Ft Myer Dr | Rosslyn Metro / Wilson Blvd & Ft Myer Dr  |\r\n",
      "|  0h 28m 7s  | Rosslyn Metro / Wilson Blvd & Ft Myer Dr | Rosslyn Metro / Wilson Blvd & Ft Myer Dr  |\r\n",
      "|  0h 26m 4s  | 4th & E St SW                            | Constitution Ave & 2nd St NW/DOL          |\r\n",
      "|  0h 26m 11s | 4th & E St SW                            | Constitution Ave & 2nd St NW/DOL          |\r\n",
      "|  0h 26m 10s | 4th & E St SW                            | Constitution Ave & 2nd St NW/DOL          |\r\n",
      "|  0h 26m 26s | 4th & E St SW                            | Constitution Ave & 2nd St NW/DOL          |\r\n",
      "|  0h 4m 54s  | King St & Patrick St                     | Market Square / King St & Royal St        |\r\n",
      "|  0h 4m 36s  | 14th & Belmont St NW                     | 17th & Corcoran St NW                     |\r\n",
      "|-------------+------------------------------------------+-------------------------------------------|\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2013-Q4-Trips-History-Data2.csv.gz | head | csvcut -c 1,3,5 | csvlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use csvcut this way to rearrange the data from 2013-Q4 and later into the same column pattern as the earlier data.  It's a pretty great tool, and csvkit provides a lot more than just that - definitely take some time to [read the csvkit docs](http://csvkit.readthedocs.org/en/0.9.0/) and try the tutorial.\n",
    "\n",
    "For now, though, let's get on with making a map.  To do this, we will pull all the data together, and then split it up into equal chunks, for three reasons.  First, the per-quarter files have a variable amount of data in them, as bike usage and bike/dock capacity grew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5399600\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 201*.csv.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about 5.4 million rides.  And there were a lot more in the later ones than in the first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  117693\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2010-Q4-Trips-History-Data.csv.gz | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  848190\r\n"
     ]
    }
   ],
   "source": [
    "gzcat 2013-Q3-Trips-History-Data.csv.gz | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, if we count all the stations up at the end of each quarter (of which we only have 12) and drew a map where the docks lit up when they were installed, it wouldn't be very dynamic, as they would light up in bunches 12 times and then stop.  If we instead split the files up into smaller but even-sized (by number of rides) chunks we can see the new docks light up with a little more granularity.\n",
    "\n",
    "A second reason to split the data up into multiple smaller files is that we can grab summary statistics over those smaller chunks, and use those to study the data.\n",
    "\n",
    "The third reason to split the data up into smaller files is that we can operate on them in parallel.  If it's all just one big file with six million lines, or a handful of mid-sized files, each long file will take a while to process.  If you have a machine with multiple CPUs and cores, though, you can use them all at once to churn through the data a lot faster.  We'll take advantage of that soon.\n",
    "\n",
    "For now, let's extract only the start station column, combine it all into one file, then split that up into even, small chunks.  First the extract-and-combine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for f in 201*.gz\n",
    "do\n",
    "  gzcat $f | csvcut -c 4 >> combined-rides.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That slices out the start station column from each of the files and appends it to combined-rides.txt, which should be about the same length as what we saw before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5399600 combined-rides.txt\r\n"
     ]
    }
   ],
   "source": [
    "wc -l combined-rides.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect - exactly the same.  But it's a big file, so let's compress it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 dchud  405867214   153M Mar 24 23:52 combined-rides.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh combined-rides.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gzip combined-rides.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 dchud  405867214    12M Mar 25 00:01 combined-rides.txt.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh combined-rides.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split it, using ```split```, into a few hundred small files with 20,000 rides each, using a simple naming convention of \"rides20k-\" plus an ordered pair of letters that ```split``` will generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gzcat combined-rides.txt.gz | split -l 20000 - rides20k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's see what we've got, looking at the first and last files.  This is a simple check to see if the data comes out consistently.  We'll sort them each, then count the unique lines, and then reverse sort that by their counts.  Key commands here are ```sort``` and ```uniq```... easy to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 744 Massachusetts Ave & Dupont Circle NW (31200)\r\n",
      " 714 Adams Mill & Columbia Rd NW (31104)\r\n",
      " 639 15th & P St NW (31201)\r\n",
      " 607 14th & V St NW (31101)\r\n",
      " 543 20th St & Florida Ave NW (31110)\r\n",
      " 543 17th & Corcoran St NW (31214)\r\n",
      " 474 16th & U St NW (31229)\r\n",
      " 469 Eastern Market Metro / Pennsylvania Ave & 7th St SE (31613)\r\n",
      " 465 Lamont & Mt Pleasant NW (31107)\r\n",
      " 450 Park Rd & Holmead Pl NW (31602)\r\n"
     ]
    }
   ],
   "source": [
    "sort rides20kaa | uniq -c | sort -rn | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 520 Massachusetts Ave & Dupont Circle NW\r\n",
      " 418 Columbus Circle / Union Station\r\n",
      " 393 Lincoln Memorial\r\n",
      " 363 15th & P St NW\r\n",
      " 292 Jefferson Dr & 14th St SW\r\n",
      " 276 17th & Corcoran St NW\r\n",
      " 268 Thomas Circle\r\n",
      " 246 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      " 239 New Hampshire Ave & T St NW\r\n",
      " 238 14th & V St NW\r\n"
     ]
    }
   ],
   "source": [
    "sort rides20kkj | uniq -c | sort -rn | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oof, see the problem?  Some of the station names have an identifier in them, others don't.  We could go through all the files, figure out which ones have them, and remove them, or we could just use ```sed``` to find and remove that pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 744 Massachusetts Ave & Dupont Circle NW\r\n",
      " 714 Adams Mill & Columbia Rd NW\r\n",
      " 639 15th & P St NW\r\n",
      " 607 14th & V St NW\r\n",
      " 543 20th St & Florida Ave NW\r\n",
      " 543 17th & Corcoran St NW\r\n",
      " 474 16th & U St NW\r\n",
      " 469 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      " 465 Lamont & Mt Pleasant NW\r\n",
      " 450 Park Rd & Holmead Pl NW\r\n"
     ]
    }
   ],
   "source": [
    "sed -E 's/ \\([0-9]+)//' rides20kaa | sort | uniq -c | sort -rn | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahh, much cleaner.  Now we don't have to worry about where in the files the convention changed (although it's possible there are other quirks like this).\n",
    "\n",
    "Let's re-run that split with the ```sed``` piece in the mix so these files will come out cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "gzcat combined-rides.txt.gz | sed -E 's/ \\([0-9]+)//' | split -l 20000 - rides20k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can churn through all of them really quickly using \n",
    "[parallel](http://www.gnu.org/software/parallel/), which takes a list of files and runs the \n",
    "same command over all of them, using as many processors as you can give it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "ls rides20k* | parallel -j+0 \"sort {} | uniq -c | sort -rn > {}-counts.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a little magic happens here... add the ```--eta``` option to parallel after the ```-j+0``` to see some updates on your command line as it cranks through, and for more, see the [GNU Parallel site](http://www.gnu.org/software/parallel/) or the video in [this writeup](http://unethicalblogger.com/2010/11/11/gnu-parallel-changed-my-life.html) for some more info about how it works)\n",
    "\n",
    "...and when that finishes, we'll have a bunch of files like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 744 Massachusetts Ave & Dupont Circle NW\r\n",
      " 714 Adams Mill & Columbia Rd NW\r\n",
      " 639 15th & P St NW\r\n",
      " 607 14th & V St NW\r\n",
      " 543 20th St & Florida Ave NW\r\n",
      " 543 17th & Corcoran St NW\r\n",
      " 474 16th & U St NW\r\n",
      " 469 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      " 465 Lamont & Mt Pleasant NW\r\n",
      " 450 Park Rd & Holmead Pl NW\r\n"
     ]
    }
   ],
   "source": [
    "head rides20kaa-counts.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data's all chunked up consistently, let's get it onto a map.  We can get coordinates of the docks from the live feed of station status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2015-03-25 00:27:35--  http://www.capitalbikeshare.com/data/stations/bikeStations.xml\r\n",
      "Resolving www.capitalbikeshare.com... 69.20.33.150\r\n",
      "Connecting to www.capitalbikeshare.com|69.20.33.150|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 158907 (155K) [text/xml]\r\n",
      "Saving to: 'bikeStations.xml'\r\n",
      "\r\n",
      "\r",
      "bikeStations.xml      0%[                      ]       0  --.-KB/s             \r",
      "bikeStations.xml    100%[=====================>] 155.18K   887KB/s   in 0.2s   \r\n",
      "\r\n",
      "2015-03-25 00:27:35 (887 KB/s) - 'bikeStations.xml' saved [158907/158907]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "wget http://www.capitalbikeshare.com/data/stations/bikeStations.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n",
      "<stations lastUpdate=\"1427257615762\" version=\"2.0\">\r\n",
      "  <station>\r\n",
      "    <id>1</id>\r\n",
      "    <name>20th &amp; Bell St</name>\r\n",
      "    <terminalName>31000</terminalName>\r\n",
      "    <lastCommWithServer>1427250282118</lastCommWithServer>\r\n",
      "    <lat>38.8561</lat>\r\n",
      "    <long>-77.0512</long>\r\n",
      "    <installed>true</installed>\r\n",
      "    <locked>false</locked>\r\n",
      "    <installDate>0</installDate>\r\n",
      "    <removalDate/>\r\n",
      "    <temporary>false</temporary>\r\n",
      "    <public>true</public>\r\n",
      "    <nbBikes>9</nbBikes>\r\n",
      "    <nbEmptyDocks>2</nbEmptyDocks>\r\n",
      "    <latestUpdateTime>1427250282118</latestUpdateTime>\r\n",
      "  </station>\r\n",
      "  <station>\r\n",
      "I/O error : Broken pipe\r\n",
      "I/O error : write error\r\n"
     ]
    }
   ],
   "source": [
    "xmllint --format bikeStations.xml | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stands to reason that we could use this recent list of stations and their coordinates to get most of the coordinates of lists of stations from the past.  We might miss a few because stations have been removed or moved slightly, but overall it should tell the big picture story pretty well.\n",
    "\n",
    "We could parse this xml, but it's a little easier to parse json, so let's convert it over using [xml2json](https://www.npmjs.com/package/xml2json):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "xml2json < bikeStations.xml > bike-stations.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "\t\"stations\": {\r\n",
      "\t\t\"lastUpdate\": \"1427257615762\",\r\n",
      "\t\t\"version\": \"2.0\",\r\n",
      "\t\t\"station\": [\r\n",
      "\t\t\t{\r\n",
      "\t\t\t\t\"id\": {\r\n",
      "\t\t\t\t\t\"$t\": \"1\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"name\": {\r\n",
      "\t\t\t\t\t\"$t\": \"20th & Bell St\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"terminalName\": {\r\n",
      "\t\t\t\t\t\"$t\": \"31000\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"lastCommWithServer\": {\r\n",
      "\t\t\t\t\t\"$t\": \"1427250282118\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"lat\": {\r\n",
      "\t\t\t\t\t\"$t\": \"38.8561\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"long\": {\r\n",
      "\t\t\t\t\t\"$t\": \"-77.0512\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"installed\": {\r\n",
      "\t\t\t\t\t\"$t\": \"true\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"locked\": {\r\n",
      "\t\t\t\t\t\"$t\": \"false\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"installDate\": {\r\n",
      "\t\t\t\t\t\"$t\": \"0\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"removalDate\": {},\r\n",
      "\t\t\t\t\"temporary\": {\r\n",
      "\t\t\t\t\t\"$t\": \"false\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"public\": {\r\n",
      "\t\t\t\t\t\"$t\": \"true\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"nbBikes\": {\r\n",
      "\t\t\t\t\t\"$t\": \"9\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"nbEmptyDocks\": {\r\n",
      "\t\t\t\t\t\"$t\": \"2\"\r\n",
      "\t\t\t\t},\r\n",
      "\t\t\t\t\"latestUpdateTime\": {\r\n",
      "\t\t\t\t\t\"$t\": \"1427250282118\"\r\n",
      "\t\t\t\t}\r\n",
      "\t\t\t},\r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "cat bike-stations.json | head -50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Turning this into a csv with just the station names, lats, and lons, just takes a little python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "\n",
    "d = json.load(open('bike-stations.json'))\n",
    "\n",
    "fp = open('station-locations.txt', 'wb')\n",
    "for station in d['stations']['station']:\n",
    "    name = station['name']['$t']\n",
    "    lat = station['lat']['$t']\n",
    "    lon = station['long']['$t']\n",
    "    fp.write(','.join([name, lat, lon]) + '\\n')\n",
    "\n",
    "fp.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-------------------------------------------------------------------+------------+-------------|\r\n",
      "|  20th & Bell St                                                   | 38.8561    | -77.0512    |\r\n",
      "|-------------------------------------------------------------------+------------+-------------|\r\n",
      "|  18th & Eads St.                                                  | 38.85725   | -77.05332   |\r\n",
      "|  20th & Crystal Dr                                                | 38.8564    | -77.0492    |\r\n",
      "|  15th & Crystal Dr                                                | 38.86017   | -77.049593  |\r\n",
      "|  Aurora Hills Community Ctr/18th & Hayes St                       | 38.857866  | -77.05949   |\r\n",
      "|  Pentagon City Metro / 12th & S Hayes St                          | 38.862303  | -77.059936  |\r\n",
      "|  S Joyce & Army Navy Dr                                           | 38.8637    | -77.0633    |\r\n",
      "|  Crystal City Metro / 18th & Bell St                              | 38.8573    | -77.0511    |\r\n"
     ]
    }
   ],
   "source": [
    "python convertstations.py\n",
    "csvlook station-locations.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got station names, counts, and lats/lons, and we just need to zip it all up.  A little more python will do, here as ```mapify.py```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "stations = {}\n",
    "for line in open('station-locations.txt'):\n",
    "    loc, lat, lon = line.strip().split(',')\n",
    "    stations[loc] = (lat, lon)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = []\n",
    "    fn = sys.argv[1]\n",
    "    for line in open(fn):\n",
    "        count, loc = line.strip().split(' ', 1)\n",
    "        try:\n",
    "            lat, lon = stations[loc]\n",
    "            rec = {\n",
    "                'loc': loc,\n",
    "                'lat': float(lat),\n",
    "                'lon': float(lon),\n",
    "                'count': int(count),\n",
    "                'percent': float(count) / 300\n",
    "                }\n",
    "            data.append(rec)\n",
    "        except:\n",
    "            pass\n",
    "    json.dump(data, open(fn + '.json', 'wb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which we can run through with ```parallel``` again, adding coordinates to each file and then saving them back out again as json for easy loading over the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        5.24 real        10.33 user         4.84 sys\r\n"
     ]
    }
   ],
   "source": [
    "ls rides20k??-counts.txt | time parallel -j+0 './mapify.py {}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  {\r\n",
      "    \"lat\": 38.9101,\r\n",
      "    \"loc\": \"Massachusetts Ave & Dupont Circle NW\",\r\n",
      "    \"percent\": 2.48,\r\n",
      "    \"lon\": -77.0444,\r\n",
      "    \"count\": 744\r\n",
      "  },\r\n",
      "  {\r\n",
      "    \"lat\": 38.922925,\r\n",
      "    \"loc\": \"Adams Mill & Columbia Rd NW\",\r\n",
      "    \"percent\": 2.38,\r\n",
      "    \"lon\": -77.042581,\r\n",
      "    \"count\": 714\r\n",
      "  },\r\n"
     ]
    }
   ],
   "source": [
    "cat rides20kaa-counts.txt.json | jq '.' | head -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we then just need to glue back into one big file to send all at once, again with a little python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "d = []\n",
    "for fn in sorted(os.listdir('.')):\n",
    "    if fn.startswith('rides20k') and fn.endswith('.txt.json'):\n",
    "        d.append(json.load(open(fn)))\n",
    "\n",
    "json.dump(d, open('combined.json', 'wb'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "python combine.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 dchud  405867214   4.7M Apr 13 15:26 combined.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -lht combined.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\r\n",
      "  [\r\n",
      "    {\r\n",
      "      \"lat\": 38.9101,\r\n",
      "      \"loc\": \"Massachusetts Ave & Dupont Circle NW\",\r\n",
      "      \"percent\": 2.48,\r\n",
      "      \"count\": 744,\r\n",
      "      \"lon\": -77.0444\r\n",
      "    },\r\n",
      "    {\r\n",
      "      \"lat\": 38.922925,\r\n",
      "      \"loc\": \"Adams Mill & Columbia Rd NW\",\r\n",
      "      \"percent\": 2.38,\r\n",
      "      \"count\": 714,\r\n",
      "      \"lon\": -77.042581\r\n",
      "    },\r\n"
     ]
    }
   ],
   "source": [
    "cat combined.json | jq '.' | head -16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...and there you have it, counts of bikeshare rides in 30,000 chunks, with counts and a latitude and longitude for ride station origins, and suitable for feeding into a simple map viz, if rather unwieldy in size/shape."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
